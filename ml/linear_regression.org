#+TITLE: Multivariate Linear Regression

* Formulation

** Notation
   x^i_j = feature j in i^th training example
   x_i = input features in i^th training example
   m = number of training examples
   n = the number of features

** Hypothesis function
   $h_\theta(x) = \sum_{j=0}^n \theta_j x_j = \theta^Tx = \theta \cdot
   x$ 

   Where $x_0 = 1$ always.

** Cost function
   $J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^i) - y^i)^2$

* Gradient descent
  - Iterative algorithm to minimize the cost function J
  - Repeat until convergence for j = 0 to n
  - (Update all values simultaneously)

  $\theta_j := \theta_j - \alpha \frac{\partial}{\partial x_j}J(\theta)$

  $\theta_j := \theta_j - \alpha
  \frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^i) - y^i)\cdot x_j^i$

