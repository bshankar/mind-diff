#+TITLE: Neural Networks
#+STARTUP: latexpreview

* Motivation
  Some problems require highly non-linear hypotheses. But we cannot use
  linear/logistic regression because the number of features required
  will be too high

  - If we have 50x50 images to recognize cars
  - And we include all quadratic terms: 2500^2/2 ~ 3 million features

  - Inspired by the brain
  - One learning algorithm hypothesis

* Model representation

#+BEGIN_SRC dot :file logistic_regression.png :cmdline -Kdot -Tpng
digraph logistic_regression {
  rankdir=LR
  splines=line

  node [fixedsize=true, label=""]
  edge [arrowhead=none]

  subgraph cluster_0 {
		color=white
		node [style=solid,color=blue4, shape=circle]
		x1 x2 x3
		label = "Input Layer"
	}

	subgraph cluster_1 {
		color=white
		node [style=solid,color=seagreen2, shape=circle]
		O
		label="Output Layer"
	}
  x1 -> O
  x2 -> O
  x3 -> O
}
#+END_SRC

#+RESULTS:
[[file:logistic_regression.png]]
  - If $h_{\theta}(x) = \frac{1}{1 + e^{-\theta^TX}}$ then it is a
    Sigmoid activation function

  - Theta is the parameters/ weights

  - x0 is bias neuron = 1

#+BEGIN_SRC dot :file neural_network_example.png :cmdline -Kdot -Tpng
digraph neural_network {
  rankdir=LR
  splines=line

  node [fixedsize=true, label=""]
  edge [arrowhead=none]

  subgraph cluster_0 {
		color=white
		node [style=solid,color=blue4, shape=circle]
		x1 x2 x3
		label = "Input Layer (1)"
	}

  subgraph cluster_1 {
    color=white
    node [style=solid, color=coral, shape=circle]
    a1 a2 a3
    label = "Hidden Layer (2)"
  }

	subgraph cluster_2 {
		color=white
		node [style=solid,color=seagreen2, shape=circle]
		O
		label="Output Layer (3)"
	}
  x1 -> a1
  x1 -> a2
  x1 -> a3
  x2 -> a1
  x2 -> a2
  x2 -> a3
  x3 -> a1
  x3 -> a2
  x3 -> a3

  a1 -> O
  a2 -> O
  a3 -> O
}
#+END_SRC

#+RESULTS:
[[file:neural_network_example.png]]

  - Hidden layers, input layer, output layer

  - each layer's neuron connects to all? neurons in its neighboring layers

* Forward propagation

  - $a_i^{(j)}$ : activation of unit i in layer j

  - $\Theta^{(j)}$ : matrix of weights controlling function mapping
    from layer j to j + 1

  - If network $s_j$ units in layer j and $s_{j + 1}$ in layer j + 1 then
    $\Theta^{(j)}$ will have dimensions of $s_{j + 1} \times (s_j + 1)$

  - Here's how to compute the hypothesis for the above neural network

  \begin{align*}
  a_1^{(2)} &= g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
  a_2^{(2)} &= g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
  a_3^{(2)} &= g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
  h_\Theta(x) &= a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})
  \end{align*}

  -  Forward propagation: Vectorized implementation

  $h_{\theta}(x) = a^{(j + 1)} = g(\Theta^{(j)} a^{(j)})$

  - the last 2 layers combined look like logistic regression but the
    inputs to this logistic regression are some complicated non-linear
    hypotheses.

* Cost function
  - L is number of layers in network
  - s_l is the number of units in layer l (excluding bias)
  - s_L = K = 1 in binary classification problem
  - K >= 3 in multiclass classification problem

  $J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2$

* Backpropagation Algorithm

** To be computed

   - $J(\Theta)$

   - $\frac{\partial}{\partial\Theta^l_{ij}}J(\Theta)$

** Error

   - no delta for layer 1

   - $\delta_j^{(l)}$ = Error in node j in layer l.

   - -$\delta_j^{(L)} = a_j^{(L)} - y_j$ and $a_j^{(L)} = h_{\theta}(x)_j$

   - $\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l + 1)}.*g'(z^{(l)})$

   - $g'(z^{(l)}) = a^{(l)}.*(1 - a^{(l)})$

** Gradient

   - $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)}
     \delta_i^{(l+1)}$

   - Vectorized:  $\Delta^{(l)} := \Delta^{(l)} +
     \delta^{(l+1)}(a^{(l)})^T$

   - $D_{ij}^{(l)} = \frac{1}{m} \left ( \Delta_{ij}^{(l)} + \lambda
     \Theta_{ij}^{(l)} \right )$ and $\lambda = 0$ for $j = 0$

   - $\frac{\partial}{\partial\Theta^l_{ij}}J(\Theta) = D_{ij}^{(l)}$
