#+TITLE: Neural Networks
#+STARTUP: latexpreview

* Motivation
  Some problems require highly non-linear hypotheses. But we cannot use
  linear/logistic regression because the number of features required
  will be too high

  - If we have 50x50 images to recognize cars
  - And we include all quadratic terms: 2500^2/2 ~ 3 million features

  - Inspired by the brain
  - One learning algorithm hypothesis

* Model representation

#+BEGIN_SRC dot :file logistic_regression.png :cmdline -Kdot -Tpng
digraph logistic_regression {
  rankdir=LR
  splines=line

  node [fixedsize=true, label=""]
  edge [arrowhead=none]

  subgraph cluster_0 {
		color=white
		node [style=solid,color=blue4, shape=circle]
		x1 x2 x3
		label = "Input Layer"
	}

	subgraph cluster_1 {
		color=white
		node [style=solid,color=seagreen2, shape=circle]
		O
		label="Output Layer"
	}
  x1 -> O
  x2 -> O
  x3 -> O
}
#+END_SRC

#+RESULTS:
[[file:logistic_regression.png]]
  - If $h_{\theta}(x) = \frac{1}{1 + e^{-\theta^TX}}$ then it is a
    Sigmoid activation function

  - Theta is the parameters/ weights

  - x0 is bias neuron = 1

#+BEGIN_SRC dot :file neural_network_example.png :cmdline -Kdot -Tpng
digraph neural_network {
  rankdir=LR
  splines=line

  node [fixedsize=true, label=""]
  edge [arrowhead=none]

  subgraph cluster_0 {
		color=white
		node [style=solid,color=blue4, shape=circle]
		x1 x2 x3
		label = "Input Layer (1)"
	}

  subgraph cluster_1 {
    color=white
    node [style=solid, color=coral, shape=circle]
    a1 a2 a3
    label = "Hidden Layer (2)"
  }

	subgraph cluster_2 {
		color=white
		node [style=solid,color=seagreen2, shape=circle]
		O
		label="Output Layer (3)"
	}
  x1 -> a1
  x1 -> a2
  x1 -> a3
  x2 -> a1
  x2 -> a2
  x2 -> a3
  x3 -> a1
  x3 -> a2
  x3 -> a3

  a1 -> O
  a2 -> O
  a3 -> O
}
#+END_SRC

#+RESULTS:
[[file:neural_network_example.png]]

  - Hidden layers, input layer, output layer

  - each layer's neuron connects to all? neurons in its neighboring layers

* Forward propagation

  - $a_i^{(j)}$ : activation of unit i in layer j

  - $\Theta^{(j)}$ : matrix of weights controlling function mapping
    from layer j to j + 1

  - If network $s_j$ units in layer j and $s_{j + 1}$ in layer j + 1 then
    $\Theta^{(j)}$ will have dimensions of $s_{j + 1} \times (s_j + 1)$

  - Here's how to compute the hypothesis for the above neural network

  \begin{align*}
  a_1^{(2)} &= g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\
  a_2^{(2)} &= g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\
  a_3^{(2)} &= g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\
  h_\Theta(x) &= a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})
  \end{align*}

  -  Forward propagation: Vectorized implementation

  $h_{\theta}(x) = a^{(j + 1)} = g(\Theta^{(j)} a^{(j)})$

  - the last 2 layers combined look like logistic regression but the
    inputs to this logistic regression are some complicated non-linear
    hypotheses.

* Cost function
  - L is number of layers in network
  - s_l is the number of units in layer l (excluding bias)
  - s_L = K = 1 in binary classification problem
  - K >= 3 in multiclass classification problem

  $J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2$

* Backpropagation Algorithm

** To be computed

   - $J(\Theta)$

   - $\frac{\partial}{\partial\Theta^l_{ij}}J(\Theta)$

** Error

   - no delta for layer 1

   - $\delta_j^{(l)}$ = Error in node j in layer l.

   - -$\delta_j^{(L)} = a_j^{(L)} - y_j$ and $a_j^{(L)} = h_{\theta}(x)_j$

   - $\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l + 1)}.*g'(z^{(l)})$

   - $g'(z^{(l)}) = a^{(l)}.*(1 - a^{(l)})$

** Gradient

   - $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)}
     \delta_i^{(l+1)}$

   - Vectorized:  $\Delta^{(l)} := \Delta^{(l)} +
     \delta^{(l+1)}(a^{(l)})^T$

   - $D_{ij}^{(l)} = \frac{1}{m} \left ( \Delta_{ij}^{(l)} + \lambda
     \Theta_{ij}^{(l)} \right )$ and $\lambda = 0$ for $j = 0$

   - $\frac{\partial}{\partial\Theta^l_{ij}}J(\Theta) = D_{ij}^{(l)}$

* Backpropagation tips

** Unrolling parameters
   - To use fminunc(@costFunction, initialTheta, options) which
     assumes costFunction(gradient, theta) and initialTheta are vectors

   - But our
     $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$
     $D^{(1)}, D^{(2)}, D^{(3)}$ are matrices.

   - Suppose theta1 is 10x11, theta2 is 10x11 and theta3 is 1x11

   - We can unroll all thetas and put them in a vector and get the
     original thetas back

     #+BEGIN_SRC octave
     % unroll
     thetaVec = [theta1(:); theta2(:); theta3(:);]

     % get back the original thetas
     theta1 = reshape(thetaVec(1:110), 10, 11)
     theta2 = reshape(thetaVec(111:220, 10, 11))
     theta3 = reshape(thetaVec(221:231, 1, 11))
     #+END_SRC

** Gradient checking
   - Numerically approximate the gradient and check if it is almost
     equal to D. Then disable it before using
     backpropagation. Otherwise it will be very /slow/.

     $\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$

** Random initialization
   - If we set $\Theta_{ij}^{(l)} = r$ for all i,j,l
   - $a_1^{(2)} = a_2^{(2)}$, same with deltas and

     $\frac{\partial}{\partial\Theta_{01}^{(1)}} J(\Theta) =
     \frac{\partial}{\partial\Theta_{02}^{(1)}} J(\Theta)$ so all the

     units in each inner layer keep computing the same parameter

   - For symmetry breaking we set initial theta to a random value in $[-\epsilon, \epsilon]$

* Putting it all together
** Architecture
  - Number of input units = dimension of features
  - Number of output units = number of classes
  - Number of hidden units/layer = More the better (but more = slow)
  - Default: 1 hidden layer. If > 1, use same # of units in every
    hidden layer

** Training
   - Randomly initialize the weights
   - Implement forward propagation to get h for any x
   - Implement the cost function
   - Implement backpropagation to compute the partial derivatives
   - Check backpropagation using gradient checking. Then /disable/ it.
   - Use gradient descent/ built-in optimization function to minimize J.

   - J is /not convex/. We can end up at a local minimum.
