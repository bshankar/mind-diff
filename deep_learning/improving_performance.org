#+STARTUP: latexpreview

* Guidelines
  - Training, Cross-validation/dev set, test set. (60/20/20% was the
     old standard)
  - In the 'Big Data' era, like m = 1,000,000 we can use 10k for each
     of dev and test sets.
  - dev, test sets should come from the same distribution.

  - Not having a test set might be okay.

* Bias / Variance
  - If optimal (Bayes) error is almost 0% for a problem (Cat vs Non
    Cat in humans)

  | Training set Error |            1% |       15% |                 0.5% |                0.5% |
  | Dev set Error      |           11% |       16% |                  30% |                  1% |
  |                    |               |           |                      |                     |
  | Diagnosis          | High variance | High Bias | High Bias & Variance | Low Bias & Variance |

* Recipe
  - Do a simple ML model and then

  #+BEGIN_SRC dot :file recipe.png :cmdline -Kdot -Tpng
  digraph recipe {
    rankdir=LR

    hb [label="High Bias?"]
    hbs [label="Bigger Network\n Train longer\n NN architecture\n (fit training set)"]
    hv [label="High Variance?"]
    hvs [label="More data\n Regularization\n NN architecture"]
    hb -> hbs [label="Y"]

    hb -> hv [label="N"]
    hv -> hvs [label="Y"]
    hv -> done [label="N"]
  }
  #+END_SRC

  #+RESULTS:
  [[file:recipe.png]]

* Regularization
  - Reduces variance

  $J(w, b) = \frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(\hat{y}^{(i)},
  y^{(i)}) + \frac{\lambda}{2m} || W ||^2$

  - We just omit b because almost all parameters are in W.

  - This is L2 regularization (most commonly used)

  # ???
  - L1 regularization doesn't square W. It makes W matrix sparse

** Dropout regularization
   - Randomly cull some nodes in the neural network (on every iteration)
   - J is not very well defined now. Calculate J without dropouts
     while plotting to check gradient descent.

*** Inverted dropout
    - Can't rely on one feature so have to spread out weights
    - It's like an adaptive form of L2 regularization
    - We can vary keep_prob between layers
    - Used frequently in *Computer Vision*

    #+BEGIN_SRC python
    d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
    a3 = np.multiply(a3, d3)
    a3 /= keep_prob
    #+END_SRC

** Other methods
*** Data augmentation
*** Early stopping
    - If dev set error goes down for a while and then rises, then stop
      iterations of gradient descent there.
    - *Orthogonalization* is lost. While trying to prevent over-fitting,
      we are not minimizing J enough.
    - Better to use L2 regularization

* Normalizing inputs
  - Center the data around the origin and spread them equally around
    the origin. Speeds up gradient descent.

  - Mean: -$\mu = \frac{1}{m}\sum_{i=1}^m X^{(i)}$

  - Variance: $\sigma^2 = \frac{1}{m} \sum_{i=1}^m X^{(i)} ** 2$

  - New features j$x = \frac{x - \mu}{\sigma^2}$
* Vanishing/Exploding Gradients
  - Deep Neural networks can have this problem
** Weight initialization
   - for large n, we need small weights. So,

   - If g(z) = ReLU(z), then we need to change things such that

     $\sigma^2(w_i) = \frac{2}{n}$ we can set

     #+BEGIN_SRC python
     W_l = np.random.randn(shape)*np.sqrt(2/n_{l-1})
     #+END_SRC

   - if g(z) is tanh(z), use Xavier initialization

     $\sqrt{\frac{1}{n^{[l-1]}}}$ or $\sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}$

* Gradient checking

  $f'(\theta) = \lim_{\epsilon \to 0} \frac{f(\theta + \epsilon) -
  f(\theta - \epsilon)}{2\epsilon} + O(\epsilon^2)$

  - This should be approximately equal to $d\theta$

  - E.g. use $\epsilon \approx 10^{-7}$ and check if

    $\frac{||d\theta_{approx} -
    d\theta||}{||d\theta_{approx}|| + ||d\theta||} \leq 10^{-7}$

  - Use only to debug, find where grad approx is different.
  - Don't forget regularization
  - It doesn't work with dropout. Turn it off, check, then turn on.

  - Gradient check /after/ some training because gradients may be
    approximately correct when weights are randomly initialized to
    very small values.
