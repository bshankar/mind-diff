#+STARTUP: latexpreview

* Notation
  - The layers are denoted by [1] [2] ...
  - [1] is the input layer x1, x2, ...

  - The last layer outputs $\hat{y}$

  - We use $W^{[1]}$, $b^{[1]}$ to compute $z^{[1]} = W^{[1]}x +
    b^{[1]}$ and $a^{[1]} = \sigma(z^{[1]})$
  - and so on for [2], [3], ... until the last layer

  - Last layer's activation a will be used to compute $\mathcal{L}(a, y)$

  - $a^{[0]} = X$ is the input layer. Input layer is not counted. so a
    2-layer Neural Network has one hidden layer.

* Activation functions
  - For hidden units, after we center the data, hyperbolic tan is
    almost always is a better choice (Just a shifted Sigmoid function)

    $g(z^{[l]}) = \tanh{z^{[l]}} = \frac{e^z - e^{-z}}{e^z + e^{-z}}$

  - Use Sigmoid function for the output layer.

** Disadvantage of tanh, sigmoid
   - As Z becomes large, the gradient becomes very small and slows
     down gradient descent

   - *Rectified linear function (ReLU)* $a = \max(0, z)$ is better in that
     way. This function is not differentiable at a = 0 but, assigning
     it some derivative would work.

   - Use ReLU by default. Leaky ReLU is fine too.

** Activation function shouldn't be linear
   - Because neural networks will then output a linear activation
     function of the input
   - A big neural network with many hidden layers with a linear
     activation function and Sigmoid function at the output layer is
     no more expressive than *Logistic regression*

   - *Unless* you are doing a regression problem. Then the output layer
     can have a linear/identify activation function

* Derivatives

  $\sigma'(z) = \sigma(z) \left (1 - \sigma(z)\right )$

  $\tanh'{z} = 1 - \tanh^2{z}$

** ReLU
   $g(z) = \max(0, z)$

  $g'(z) = \begin{cases}
    0 & \mbox{if } x < 0, \\
    1 & \mbox{if } x > 0, \\
    \mbox{undefined} & \mbox{at } x = 0
  \end{cases}$

  - We can set the derivative to 1 at x = 0. Because chance of z being
    exactly 0 in gradient descent is very low.

  - Similar for leaky ReLU.

    $g(z) = max(0.01z, z)$

* Symmetry breaking
  - Initialize W's to small random values otherwise all your hidden
    units will compute the same function due to symmetry!
  - B's can be initialized to 0s

* Gradient Descent
** Forward Propagation
   - Input: $A^{[l-1]}$

   - Output: $a^{[l]}$ and cache $Z^{[l]}, W^{[l]}, b^{[l]}$

   - Repeat from first layer to last (nth)

   $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$

   $A^{[l]} = g^{[l]}(Z^{[l]})$

** Backward Propagation
   - Input: $dA^{[l]}$

   - Output: $dA^{[l-1]}, dW^{[l]}, db^{[l]}$

   - Repeat from last layer (nth) to 1st.

   $dZ^{[k]} = dA^{[l]} \star g'^{[l]}(Z^{[l]})$

   $dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1]}^T$

   $db^{[l]} = \frac{1}{m} \sum\limits_{row} dZ^{[l]}$

   $dA^{[l-1]} = \begin{cases} & W^{[l]T}dZ^{[l]} \mbox{ if } l \neq L \\
   & \frac{\partial \mathcal{L}}{\partial a} \mbox{ otherwise } \end{cases}$

* Debugging (matrix dimensions)

  $Z^{[l]}, A^{[l]} \in \boldsymbol{R}^{n^{[l]} \times m}$

  $W^{[l]}, dW^{[l]} \in \boldsymbol{R}^{n^{[l]} \times n^{[l-1]}}$

  $b^{[l]}, db^{[l]} \in \boldsymbol{R}^{n^{[l]} \times 1}$

* Why deep?
  - In face recognition for example, starting layers detect small
    features like edges, then eye, nose then put them together to form
    a face in the outer units.

  - Circuit theory: There are functions you can compute with a L-layer
    deep Neural Network that shallower networks require exponentially
    more hidden units to compute.
    E.g. XOR tree

* Hyperparameters
  - $\alpha$ (learning rate), # of iterations, # of hidden layers
    (L), # of hidden units in each, choice of activation function etc.

  - These determine the parameters W and b.

  - The empirical process: idea, code, experiment cycle
